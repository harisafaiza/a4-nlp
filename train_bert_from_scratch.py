# -*- coding: utf-8 -*-
"""train_bert_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ld2P434IFQOJEZNvXwTJjZt1jLZtSjiB
"""

!pip install datasets

import os
import torch
import torch.nn as nn
import random
from transformers import BertModel, BertTokenizer
from transformers import Trainer, TrainingArguments
from datasets import load_dataset

# Disable Weights & Biases logging
os.environ["WANDB_DISABLED"] = "true"

# Load dataset and split into train and validation
dataset = load_dataset("bookcorpus", split="train[:100000]")
train_test_split = dataset.train_test_split(test_size=0.1)
train_dataset = train_test_split["train"]
eval_dataset = train_test_split["test"]

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True)

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_eval = eval_dataset.map(tokenize_function, batched=True)

# Add random labels since BookCorpus is unlabeled
def add_labels(example):
    example["labels"] = random.randint(0, 1)  # Binary classification (0 or 1)
    return example

tokenized_train = tokenized_train.map(add_labels)
tokenized_eval = tokenized_eval.map(add_labels)

# Define a BERT-based model with loss computation
class CustomBERTModel(nn.Module):
    def __init__(self):
        super(CustomBERTModel, self).__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)  # Binary classification
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        logits = self.classifier(outputs.pooler_output)

        loss = None
        if labels is not None:
            loss = self.loss_fn(logits, labels.view(-1))  # Ensure labels shape matches

        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}

model = CustomBERTModel()

# Training setup
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    report_to="none",
)

def data_collator(features):
    batch = {key: torch.tensor([f[key] for f in features]) for key in features[0] if key in ["input_ids", "attention_mask", "labels"]}
    return batch

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=data_collator
)

# Train the model
trainer.train()

# Save model
model.save_pretrained("./trained_bert")