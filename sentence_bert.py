# -*- coding: utf-8 -*-
"""sentence-bert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/163wcmlMf_blG7C6t4RsfuCy_81BJ-AAr
"""

import os
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer
from transformers import Trainer, TrainingArguments
from datasets import load_dataset

# Disable Weights & Biases logging
os.environ["WANDB_DISABLED"] = "true"

# Load dataset for sentence embeddings
dataset = load_dataset("snli", split="train[:100000]")
train_test_split = dataset.train_test_split(test_size=0.1)
train_dataset = train_test_split["train"]
eval_dataset = train_test_split["test"]

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_function(examples):
    return tokenizer(examples["premise"], examples["hypothesis"], padding="max_length", truncation=True)

tokenized_train = train_dataset.map(tokenize_function, batched=True)
tokenized_eval = eval_dataset.map(tokenize_function, batched=True)

# Define Sentence-BERT model
class SentenceBERT(nn.Module):
    def __init__(self):
        super(SentenceBERT, self).__init__()
        self.bert = BertModel.from_pretrained("bert-base-uncased")
        self.classifier = nn.Linear(self.bert.config.hidden_size * 3, 3)  # 3-way classification (entailment, neutral, contradiction)
        self.loss_fn = nn.CrossEntropyLoss()

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            loss = self.loss_fn(logits, labels.view(-1))

        return {"loss": loss, "logits": logits} if loss is not None else {"logits": logits}

model = SentenceBERT()

# Training setup
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    report_to="none",
)

def data_collator(features):
    batch = {key: torch.tensor([f[key] for f in features]) for key in features[0] if key in ["input_ids", "attention_mask", "labels"]}
    return batch

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=data_collator
)

# Train the model
trainer.train()

# Save model
model.save_pretrained("./trained_sentence_bert")